{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fdf449c-18e7-4dcc-a194-9de011757c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93b337e-75d6-4f3c-8375-038ae34f790a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import skimage\n",
    "import toml\n",
    "import sleap_io as sio\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "from markovids import pcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14500ae7-3dee-4908-8be9-0f372fec5052",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/storage/home/hcoda1/4/jmarkowitz30/shared_folder/active_lab_members/markowitz_jeffrey/active_projects/quantum_dots/_analysis'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = toml.load(\"../preprocessing/config.toml\")\n",
    "config[\"dirs\"][\"analysis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d5fc45-35ab-461c-918b-6966d0277daa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_nested_value(data, key_string, delimiter='.'):\n",
    "    \"\"\"\n",
    "    Accesses a value in a nested dictionary using a string key.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The nested dictionary.\n",
    "        key_string (str): The string representing the path to the value.\n",
    "        delimiter (str, optional): The delimiter separating keys in the string. Defaults to '.'.\n",
    "\n",
    "    Returns:\n",
    "        The value at the specified path, or None if the path is invalid.\n",
    "    \"\"\"\n",
    "    keys = key_string.split(delimiter)\n",
    "    current = data\n",
    "    for key in keys:\n",
    "        if isinstance(current, dict) and key in current:\n",
    "            current = current[key]\n",
    "        else:\n",
    "            return None\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a185ede-a814-49a5-b817-459a061386e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save_dir = \"/mnt/data/jmarkow/panels/2024-06 (QD paper)\"\n",
    "# os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6e564-6e34-41c6-b628-197e2387ee78",
   "metadata": {},
   "source": [
    "## Gather data and predicted keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89d2a8da-d603-4b67-93f6-f32b70d8c17d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pool repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdffbbb8-ccfa-4e34-91b0-1afdb2a6e17d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "root_dir = \"/storage/home/hcoda1/4/jmarkowitz30/shared_folder/active_lab_members/markowitz_jeffrey/active_projects/\"\n",
    "# model_sub_dirs = [\"keypoints_basler_nir_plexiglass_arena/sleap_training_round2/keypoints_from_manual_data_final_model/models\"]\n",
    "# model_sub_dirs = [\"keypoints_basler_nir_plexiglass_arena/sleap_training_round2/keypoints_from_manual_data_parameter_sweep/models\",\n",
    "#                   \"keypoints_basler_nir_plexiglass_arena/sleap_training_round2/keypoints_from_manual_data_parameter_sweep_part2/models\"]\n",
    "# model_sub_dirs = [\"keypoints_basler_nir_plexiglass_arena/sleap_training_round2/keypoints_from_manual_data_subsample/models\",\n",
    "#                   \"keypoints_basler_nir_plexiglass_arena/sleap_training_round2/keypoints_from_manual_data_subsample_morerepeats/models\"]\n",
    "model_sub_dirs = [\"keypoints_basler_nir_plexiglass_arena/sleap_training_round2/keypoints_from_manual_data_subsample_reflectanceonly/models\"]\n",
    "\n",
    "# model_sub_dirs = [\"keypoints_basler_nir_plexiglass_arena/sleap_training_round2/keypoints_from_manual_data_different_modalities/models\"]\n",
    "\n",
    "# model_sub_dirs = [\"keypoints_basler_nir_plexiglass_arena/sleap_training_round2/keypoints_from_manual_data_different_modalities_kneejoints/models\"]\n",
    "# model_sub_dirs = [\"keypoints_basler_nir_plexiglass_arena/sleap_training_round2/keypoints_from_manual_data_final_model_kneejoints/models\"]\n",
    "\n",
    "# model_sub_dirs = [\"keypoints_basler_nir_plexiglass_arena/sleap_training_round2/keypoints_from_manual_data_different_modalities_bottomup/models\"]\n",
    "\n",
    "model_dirs = []\n",
    "for _model_sub_dir in model_sub_dirs:\n",
    "    model_dirs += [_dir for _dir in sorted(glob(os.path.join(root_dir,_model_sub_dir,\"*_instance\"))) if os.path.isdir(_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aa888be-a59c-4f76-bb27-02468e69cd25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"kneejoints\" in model_sub_dirs[0]:\n",
    "    training_fname = os.path.join(root_dir, \"keypoints_basler_nir_plexiglass_arena/sleap_training_round2/_labels/basler-nir-plexiglass-arena-keypoints-fused-kneejoints_weights-(1.0, 0.0)_bpass-None.slp\")    \n",
    "else:\n",
    "    training_fname = os.path.join(root_dir, \"keypoints_basler_nir_plexiglass_arena/sleap_training_round2/_labels/basler-nir-plexiglass-arena-keypoints-fused-round2_weights-(1.0, 0.0)_bpass-None.slp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd5504d6-7367-4b52-b2ea-a8ef3f3d337a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_file = os.path.join(root_dir, model_sub_dirs[0], \"../aggregated_results.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd8c4e37-d617-41b1-9c09-ad85768f310e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fluo_labels = sio.load_slp(training_fname)\n",
    "skeleton = fluo_labels.skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1980973a-9039-4482-ab32-4e4bb10c6aec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff37a532d4f48b2a8e887462f165477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fluo_ims = [_label.image for _label in tqdm(fluo_labels.labeled_frames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f398031e-6817-418f-8144-1e10e3ff2572",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_types = [\"train\",\"val\",\"test\"] # make sure indices line up between file and metadata types since we just zip\n",
    "metadata_types = [\"training\",\"validation\",\"test\"]\n",
    "# file_types = [\"test\"]\n",
    "# metadata_types = [\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df6c721-87e5-49d6-93f6-aeb72587b3bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_params = {\n",
    "    \"head_output_stride\": \"model.heads.single_instance.output_stride\",\n",
    "    \"head_sigma\": \"model.heads.single_instance.sigma\",\n",
    "    \"backbone_output_stride\": \"model.backbone.unet.output_stride\",\n",
    "    \"backbone_filters\": \"model.backbone.unet.filters\",\n",
    "    \"backbone_filters_rate\": \"model.backbone.unet.filters_rate\",\n",
    "    \"backbone_max_stride\": \"model.backbone.unet.max_stride\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19eb0836-5204-4061-ad65-46aa9c745ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training_config[\"data\"][\"labels\"][\"validation_inds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9d301c6-be7a-491e-b2fb-d6af86c69bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_kpoint_gt_df(\n",
    "    model_dir,\n",
    "    fluo_ims,\n",
    "    file_types=file_types,\n",
    "    metadata_types=metadata_types,\n",
    "    search_radius=10,\n",
    "    incl_other_node_thresh=5,\n",
    "    save_params=save_params,\n",
    "):\n",
    "\n",
    "    search_obj = skimage.morphology.disk(radius=search_radius).astype(\"float\")\n",
    "    search_y, search_x = np.where(search_obj > 0)\n",
    "    center = search_obj.shape[0] // 2\n",
    "    xx = np.arange(search_obj.shape[1] + 1)\n",
    "    yy = np.arange(search_obj.shape[0] + 1)\n",
    "    xx -= center\n",
    "    yy -= center\n",
    "\n",
    "    with open(os.path.join(model_dir, \"training_config.json\")) as f:\n",
    "        training_config = json.load(f)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"initial_config.json\")) as f:\n",
    "        initial_config = json.load(f)\n",
    "\n",
    "    training_data_path = os.path.join(model_dir, \"../../\", training_config[\"data\"][\"labels\"][\"training_labels\"])\n",
    "    training_metadata_path = os.path.splitext(training_data_path)[0] + \".toml\"\n",
    "    training_metadata = toml.load(training_metadata_path)[\"segments_metadata\"]\n",
    "    gt = {}\n",
    "    for _type in file_types:\n",
    "        try:\n",
    "            gt[_type] = sio.load_slp(os.path.join(model_dir, f\"labels_gt.{_type}.slp\")).labeled_frames\n",
    "        except FileNotFoundError as e:\n",
    "            continue\n",
    "\n",
    "    pred = {}\n",
    "    for _type in file_types:\n",
    "        try:\n",
    "            pred[_type] = sio.load_slp(os.path.join(model_dir, f\"labels_pr.{_type}.slp\")).labeled_frames\n",
    "        except FileNotFoundError as e:\n",
    "            continue\n",
    "\n",
    "    use_metadata = {}\n",
    "    for _type in metadata_types:\n",
    "        try:\n",
    "            use_metadata[_type] = [training_metadata[i] for i in training_config[\"data\"][\"labels\"][f\"{_type}_inds\"]]\n",
    "        except (KeyError, TypeError) as e:\n",
    "            continue\n",
    "\n",
    "    # make sure everything is aligned\n",
    "    if (list(pred.keys()) != list(gt.keys())) or (len(pred) != len(use_metadata)):\n",
    "        print(\"misalign\")\n",
    "        return None\n",
    "\n",
    "    nframes = {}\n",
    "    for k, v in gt.items():\n",
    "        nframes[k] = len(v)\n",
    "\n",
    "    dcts = []\n",
    "    for _gts, _preds, _metadatas, _type in zip(gt.values(), pred.values(), use_metadata.values(), file_types):\n",
    "        for _dset_index, (_gt, _pred, _metadata) in enumerate(zip(_gts, _preds, _metadatas)):\n",
    "\n",
    "            try:\n",
    "                points_gt = _gt.instances[0].points\n",
    "                points_pred = _pred.instances[0].points\n",
    "            except IndexError as e:\n",
    "                continue\n",
    "\n",
    "            points_gt = {k.name: v for k, v in points_gt.items()}\n",
    "            points_pred = {k.name: v for k, v in points_pred.items()}\n",
    "\n",
    "            fluo_frame_idx = _gt.frame_idx\n",
    "            bground_sub = fluo_ims[fluo_frame_idx].squeeze()\n",
    "\n",
    "            # need to convert fluo to SNR...\n",
    "            # frame_med = np.median(bground_sub[bground_sub>0])\n",
    "            # frame_mad = np.median(np.abs(bground_sub[bground_sub>0] - frame_med))\n",
    "\n",
    "            for node, _kpoint in points_pred.items():\n",
    "\n",
    "                gt_x = points_gt[node].x\n",
    "                gt_y = points_gt[node].y\n",
    "\n",
    "                if ~np.isnan(_kpoint.x):\n",
    "\n",
    "                    try:\n",
    "                        gt_x_idx = int(np.round(gt_x))\n",
    "                        gt_y_idx = int(np.round(gt_y))\n",
    "                    except ValueError as e:\n",
    "                        continue\n",
    "\n",
    "                    kpoint_x_idx = int(np.round(_kpoint.x))\n",
    "                    kpoint_y_idx = int(np.round(_kpoint.y))\n",
    "                    kpoint_score = _kpoint.score\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        new_coords = xx + kpoint_x_idx, yy + kpoint_y_idx\n",
    "                        xrange = (new_coords[0][0], new_coords[0][-1])\n",
    "                        yrange = (new_coords[1][0], new_coords[1][-1])\n",
    "\n",
    "                        # get CoM shift relative to center, add as offset to original kpoint\n",
    "\n",
    "                        # first mask search radius, then get central blob\n",
    "                        masked_bground_sub = search_obj * bground_sub[slice(*yrange), slice(*xrange)].astype(\"float\")\n",
    "\n",
    "                        mask = pcl.fluo.get_closest_blob(masked_bground_sub)\n",
    "                        masked_bground_sub[mask == 0] = 0\n",
    "\n",
    "                        kpoint_gauss_params, kpoint_moment_params = pcl.fluo.fit_2d_gaussian_with_moments(\n",
    "                            masked_bground_sub,\n",
    "                            loss=\"linear\",\n",
    "                        )\n",
    "                        kpoint_com = [\n",
    "                            kpoint_moment_params[\"x0\"] + xrange[0],\n",
    "                            kpoint_moment_params[\"y0\"] + yrange[0],\n",
    "                        ]\n",
    "\n",
    "                        if kpoint_gauss_params is not None:\n",
    "                            kpoint_com_gauss = [\n",
    "                                kpoint_gauss_params[\"x0\"] + xrange[0],\n",
    "                                kpoint_gauss_params[\"y0\"] + yrange[0],\n",
    "                            ]\n",
    "                            kpoint_amp_gauss = kpoint_gauss_params[\"amplitude\"]\n",
    "                        else:\n",
    "                            kpoint_com_gauss = [np.nan, np.nan]\n",
    "                            kpoint_amp_gauss = np.nan\n",
    "\n",
    "                        kpoint_fluo_ave = np.nanmean(masked_bground_sub[search_y, search_x])\n",
    "                        kpoint_fluo_peak = np.nanmax(masked_bground_sub[search_y, search_x])\n",
    "\n",
    "                    except ValueError:\n",
    "\n",
    "                        kpoint_fluo_ave = np.nan\n",
    "                        kpoint_fluo_peak = np.nan\n",
    "                        kpoint_com = (np.nan, np.nan)\n",
    "\n",
    "                    _dct = {\n",
    "                        \"gt_x\": gt_x,\n",
    "                        \"gt_y\": gt_y,\n",
    "                        \"kpoint_name\": node,\n",
    "                        \"kpoint_x\": _kpoint.x,\n",
    "                        \"kpoint_y\": _kpoint.y,\n",
    "                        \"kpoint_com_x\": kpoint_com[0],\n",
    "                        \"kpoint_com_y\": kpoint_com[1],\n",
    "                        \"kpoint_com_gauss_x\": kpoint_com_gauss[0],\n",
    "                        \"kpoint_com_gauss_y\": kpoint_com_gauss[1],\n",
    "                        \"kpoint_amp_gauss\": kpoint_amp_gauss,\n",
    "                        \"kpoint_fluo_ave\": kpoint_fluo_ave,\n",
    "                        \"kpoint_fluo_peak\": kpoint_fluo_peak,\n",
    "                        \"kpoint_score\": kpoint_score,\n",
    "                        \"frame_index\": _metadata[\"frame_index\"],\n",
    "                        \"dset_index\": _dset_index,\n",
    "                        \"dset_type\": _type,\n",
    "                        \"model_dir\": model_dir,\n",
    "                        \"search_radius\": search_radius,\n",
    "                    }\n",
    "                else:\n",
    "                    # make sure we count missed keypoints...\n",
    "                    _dct = {\n",
    "                        \"gt_x\": gt_x,\n",
    "                        \"gt_y\": gt_y,\n",
    "                        \"kpoint_name\": node,\n",
    "                        \"kpoint_x\": _kpoint.x,\n",
    "                        \"kpoint_y\": _kpoint.y,\n",
    "                        \"kpoint_score\": _kpoint.score,\n",
    "                        \"frame_index\": _metadata[\"frame_index\"],\n",
    "                        \"dset_index\": _dset_index,\n",
    "                        \"dset_type\": _type,\n",
    "                        \"model_dir\": model_dir,\n",
    "                        \"search_radius\": search_radius,\n",
    "                    }\n",
    "\n",
    "                dcts.append(_dct)\n",
    "    df = pd.DataFrame(dcts)\n",
    "    for k, v in save_params.items():\n",
    "        df[k] = get_nested_value(initial_config, v)\n",
    "    for k, v in nframes.items():\n",
    "        df[f\"nframes {k}\"] = v\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd18b16d-710d-427f-ba3b-3da7c46b1a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_l2_norm(df, x1=\"kpoint_x\", x2=\"gt_x\", y1=\"kpoint_y\", y2=\"gt_y\"):\n",
    "    return np.linalg.norm([df[x1] - df[x2], df[y1] - df[y2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b87bdb6-0ffd-4f8e-83a4-a752062316e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get_kpoint_gt_df(_dir, fluo_ims, search_radius=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34b8a544-439f-449f-b1e4-4e37454872f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_radius_scan = [10.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e50c0af-0923-4ca6-83eb-f7c61247ef3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delays = []\n",
    "for _search_radius in search_radius_scan:\n",
    "    for _dir in model_dirs:\n",
    "        delays.append(joblib.delayed(get_kpoint_gt_df)(_dir, fluo_ims, search_radius=_search_radius))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "726b9b60-8c4b-4227-ba17-0ca37b59fe66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "force = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acfb01e4-4007-42a3-abef-90925594743d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get_kpoint_gt_df(model_dirs[2], fluo_ims, search_radius=_search_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b87e7e6a-2ba2-4c65-9e74-6418e6a55a25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 56 jobs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=18)]: Using backend LokyBackend with 18 concurrent workers.\n",
      "[Parallel(n_jobs=18)]: Done   5 tasks      | elapsed:   30.3s\n",
      "[Parallel(n_jobs=18)]: Done  14 tasks      | elapsed:   51.3s\n",
      "[Parallel(n_jobs=18)]: Done  27 out of  56 | elapsed:  1.3min remaining:  1.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "misalign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/conda-stage-uTuX/data_analysis/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=18)]: Done  33 out of  56 | elapsed:  1.5min remaining:  1.1min\n",
      "[Parallel(n_jobs=18)]: Done  39 out of  56 | elapsed:  1.7min remaining:   44.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "misalign\n",
      "misalign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=18)]: Done  45 out of  56 | elapsed:  2.3min remaining:   33.3s\n",
      "[Parallel(n_jobs=18)]: Done  51 out of  56 | elapsed:  2.6min remaining:   15.3s\n",
      "[Parallel(n_jobs=18)]: Done  56 out of  56 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "misalign\n",
      "misalign\n",
      "misalign\n",
      "misalign\n"
     ]
    }
   ],
   "source": [
    "if (not os.path.exists(save_file)) or force:\n",
    "    print(f\"Processing {len(delays)} jobs\")\n",
    "    results = joblib.Parallel(n_jobs=18, verbose=10)(delays)\n",
    "    result_df = pd.concat(results, ignore_index=True)\n",
    "    result_df[\"model_weight\"] = result_df[\"model_dir\"].str.extract(\"weights-(.*?)_\")\n",
    "    result_df[\"model_subsample\"] = result_df[\"model_dir\"].str.extract(\"subsample-(.*?)_\")\n",
    "    result_df[\"kpoint_gt_l2\"] = result_df.apply(\n",
    "        lambda x: get_l2_norm(x, x1=\"kpoint_x\", y1=\"kpoint_y\"), axis=1\n",
    "    )\n",
    "    result_df[\"kpoint_com_gauss_gt_l2\"] = result_df.apply(\n",
    "        lambda x: get_l2_norm(x, x1=\"kpoint_com_gauss_x\", y1=\"kpoint_com_gauss_y\"), axis=1\n",
    "    )\n",
    "    result_df[\"kpoint_com_gt_l2\"] = result_df.apply(\n",
    "        lambda x: get_l2_norm(x, x1=\"kpoint_com_x\", y1=\"kpoint_com_y\"), axis=1\n",
    "    )\n",
    "    result_df.to_parquet(save_file)\n",
    "else:\n",
    "    print(f\"Loading saved data from {save_file}\")\n",
    "    result_df = pd.read_parquet(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eeef81-762a-45c0-9be8-01b13f1c363e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis (local)",
   "language": "python",
   "name": "data_analysis_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
